#!/usr/bin/env python
"""Classes for exporting data from AFF4 to the rest of the world.

Exporters defined here convert various complex RDFValues to simple RDFValues
(without repeated fields, without recursive field definitions) that can
easily be written to a relational database or just to a set of files.
"""

import hashlib
import itertools
import stat
import time

import logging


from grr.lib import aff4
from grr.lib import rdfvalue
from grr.lib import registry
from grr.lib import threadpool
from grr.lib import utils
from grr.proto import export_pb2


class Error(Exception):
  """Errors generated by export converters."""


class NoConverterFound(Error):
  """Raised when no converter is found for particular value."""


class ExportOptions(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportOptions


class ExportedMetadata(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedMetadata


class ExportedClient(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedClient


class ExportedFile(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedFile


class ExportedRegistryKey(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedRegistryKey


class ExportedProcess(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedProcess


class ExportedNetworkConnection(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedNetworkConnection


class ExportedOpenFile(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedOpenFile


class ExportedVolatilityHandle(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedVolatilityHandle


class ExportedVolatilityMutant(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedVolatilityMutant


class ExportedNetworkInterface(rdfvalue.RDFProtoStruct):
  protobuf = export_pb2.ExportedNetworkInterface


class ExportConverter(object):
  """Base ExportConverter class."""

  __metaclass__ = registry.MetaclassRegistry

  # Type of values that this converter accepts.
  input_rdf_type = None

  def __init__(self, options=None):
    """Constructor.

    Args:
      options: ExportOptions value, which contains settings that may or
               or may not affect this converter's behavior.
    """
    super(ExportConverter, self).__init__()
    self.options = options or rdfvalue.ExportOptions()

  def Convert(self, metadata, value, token=None):
    """Converts given RDFValue to other RDFValues.

    Args:
      metadata: ExporteMetadata to be used for conversion.
      value: RDFValue to be converted.
      token: Security token.

    Yields:
      Resulting RDFValues. Empty list is a valid result and means that
      conversion wasn't possible. Resulting RDFValues may be of different
      types.
    """
    raise NotImplementedError()

  def BatchConvert(self, metadata_value_pairs, token=None):
    """Converts a batch of RDFValues at once.

    This is a default non-optimized dumb implementation. Subclasses are
    supposed to have their own optimized implementations.

    Args:
      metadata_value_pairs: a list or a generator of tuples (metadata, value),
                            where metadata is ExportedMetadata to be used for
                            conversion and value is an RDFValue to be converted.
      token: Security token:

    Yields:
      Resulting RDFValues. Empty list is a valid result and means that
      conversion wasn't possible. Resulting RDFValues may be of different
      types.
    """
    for metadata, value in metadata_value_pairs:
      for result in self.Convert(metadata, value, token):
        yield result

  @staticmethod
  def GetConvertersByValue(value):
    """Returns all converters that take given value as an input value."""
    return [cls for cls in ExportConverter.classes.itervalues()
            if cls.input_rdf_type == value.__class__.__name__]


class StatEntryToExportedFileConverter(ExportConverter):
  """Converts StatEntry to ExportedFile."""

  input_rdf_type = "StatEntry"

  MAX_CONTENT_SIZE = 1024 * 64

  def _ParseSignedData(self, signed_data, result):
    """Parses signed certificate data and updates result rdfvalue."""

  def Convert(self, metadata, stat_entry, token=None):
    """Converts StatEntry to ExportedFile.

    Does nothing if StatEntry corresponds to a registry entry and not to a file.

    Args:
      metadata: ExporteMetadata to be used for conversion.
      stat_entry: StatEntry to be converted.
      token: Security token.

    Returns:
      List or generator with resulting RDFValues. Empty list if StatEntry
      corresponds to a registry entry and not to a file.
    """
    return self.BatchConvert([(metadata, stat_entry)], token=token)

  def BatchConvert(self, metadata_value_pairs, token=None):
    """Converts a batch of StatEntry value to ExportedFile values at once.

    Args:
      metadata_value_pairs: a list or a generator of tuples (metadata, value),
                            where metadata is ExportedMetadata to be used for
                            conversion and value is a StatEntry to be converted.
      token: Security token:

    Yields:
      Resulting ExportedFile values. Empty list is a valid result and means that
      conversion wasn't possible.
    """
    filtered_pairs = []
    for metadata, stat_entry in metadata_value_pairs:
      if not stat_entry.HasField("registry_type"):
        filtered_pairs.append((metadata, stat_entry))

    aff4_paths = [stat_entry.aff4path
                  for metadata, stat_entry in metadata_value_pairs]
    fds = aff4.FACTORY.MultiOpen(aff4_paths, mode="r", token=token)
    fds_dict = dict([(fd.urn, fd) for fd in fds])

    for metadata, stat_entry in filtered_pairs:
      result = ExportedFile(metadata=metadata,
                            urn=stat_entry.aff4path,
                            basename=stat_entry.pathspec.Basename(),
                            st_mode=stat_entry.st_mode,
                            st_ino=stat_entry.st_ino,
                            st_dev=stat_entry.st_dev,
                            st_nlink=stat_entry.st_nlink,
                            st_uid=stat_entry.st_uid,
                            st_gid=stat_entry.st_gid,
                            st_size=stat_entry.st_size,
                            st_atime=stat_entry.st_atime,
                            st_mtime=stat_entry.st_mtime,
                            st_ctime=stat_entry.st_ctime,
                            st_blocks=stat_entry.st_blocks,
                            st_blksize=stat_entry.st_blksize,
                            st_rdev=stat_entry.st_rdev,
                            symlink=stat_entry.symlink)

      try:
        aff4_object = fds_dict[stat_entry.aff4path]

        hash_obj = aff4_object.Get(aff4_object.Schema.HASH)
        if hash_obj:
          result.hash_md5 = str(hash_obj.md5)
          result.hash_sha1 = str(hash_obj.sha1)
          result.hash_sha256 = str(hash_obj.sha256)

          if hash_obj.HasField("pecoff_md5"):
            result.pecoff_hash_md5 = str(hash_obj.pecoff_md5)

          if hash_obj.HasField("pecoff_sha1"):
            result.pecoff_hash_sha1 = str(hash_obj.pecoff_sha1)

          if hash_obj.HasField("signed_data"):
            self._ParseSignedData(hash_obj.signed_data[0], result)

        if self.options.export_files_contents:
          try:
            result.content = aff4_object.Read(self.MAX_CONTENT_SIZE)
            result.content_sha256 = hashlib.sha256(result.content).hexdigest()
          except (IOError, AttributeError) as e:
            logging.warning("Can't read content of %s: %s",
                            stat_entry.aff4path, e)
      except KeyError:
        pass

      yield result


class StatEntryToExportedRegistryKeyConverter(ExportConverter):
  """Converts StatEntry to ExportedRegistryKey."""

  input_rdf_type = "StatEntry"

  def Convert(self, metadata, stat_entry, token=None):
    """Converts StatEntry to ExportedRegistryKey.

    Does nothing if StatEntry corresponds to a file and nto a registry entry.

    Args:
      metadata: ExporteMetadata to be used for conversion.
      stat_entry: StatEntry to be converted.
      token: Security token.

    Returns:
      List or generator with resulting RDFValues. Empty list if StatEntry
      corresponds to a file and not to a registry entry.
    """
    if not stat_entry.HasField("registry_type"):
      return []

    result = ExportedRegistryKey(metadata=metadata,
                                 urn=stat_entry.aff4path,
                                 last_modified=stat_entry.st_mtime,
                                 type=stat_entry.registry_type)

    try:
      data = str(stat_entry.registry_data.GetValue())
    except UnicodeEncodeError:
      # If we can't represent this as a string...
      # let's just get the byte representation *shrug*
      data = stat.registry_data.GetValue()
        # Get the byte representation of the string
      data = unicode(data).encode("utf-16be")

    result.data = data
    return [result]


class ProcessToExportedProcessConverter(ExportConverter):
  """Converts Process to ExportedProcess."""

  input_rdf_type = "Process"

  def Convert(self, metadata, process, token=None):
    """Converts Process to ExportedProcess."""

    result = ExportedProcess(metadata=metadata,
                             pid=process.pid,
                             ppid=process.ppid,
                             name=process.name,
                             exe=process.exe,
                             cmdline=" ".join(process.cmdline),
                             ctime=process.ctime,
                             real_uid=process.real_uid,
                             effective_uid=process.effective_uid,
                             saved_uid=process.saved_uid,
                             real_gid=process.real_gid,
                             effective_gid=process.effective_gid,
                             saved_gid=process.saved_gid,
                             username=process.username,
                             terminal=process.terminal,
                             status=process.status,
                             nice=process.nice,
                             cwd=process.cwd,
                             num_threads=process.num_threads,
                             user_cpu_time=process.user_cpu_time,
                             system_cpu_time=process.system_cpu_time,
                             cpu_percent=process.cpu_percent,
                             rss_size=process.RSS_size,
                             vms_size=process.VMS_size,
                             memory_percent=process.memory_percent)
    return [result]


class ProcessToExportedNetworkConnectionConverter(ExportConverter):
  """Converts Process to ExportedNetworkConnection."""

  input_rdf_type = "Process"

  def Convert(self, metadata, process, token=None):
    """Converts Process to ExportedNetworkConnection."""

    for conn in process.connections:
      yield ExportedNetworkConnection(metadata=metadata,
                                      family=conn.family,
                                      type=conn.type,
                                      local_address=conn.local_address,
                                      remote_address=conn.remote_address,
                                      state=conn.state,
                                      pid=conn.pid,
                                      ctime=conn.ctime)


class ProcessToExportedOpenFileConverter(ExportConverter):
  """Converts Process to ExportedOpenFile."""

  input_rdf_type = "Process"

  def Convert(self, metadata, process, token=None):
    """Converts Process to ExportedOpenFile."""

    for f in process.open_files:
      yield ExportedOpenFile(metadata=metadata,
                             pid=process.pid,
                             path=f)


class VolatilityResultConverter(ExportConverter):
  """Base class for converting volatility results."""

  __abstract = True  # pylint: disable=g-bad-name

  input_rdf_type = "VolatilityResult"

  mapping = None
  output_rdf_cls = None

  def __init__(self):
    super(VolatilityResultConverter, self).__init__()
    if not self.mapping:
      raise ValueError("Mapping not specified.")

    if not self.output_rdf_cls:
      raise ValueError("output_rdf_cls not specified")

  def Convert(self, metadata, volatility_result, token=None):
    for section in volatility_result.sections:
      # Keep a copy of the headers and their order.
      try:
        headers = tuple(self.mapping[h.name] for h in section.table.headers)
      except KeyError as e:
        logging.warning("Unmapped header: %s", e)
        continue

      if not section.table.rows:
        logging.warning("No rows in the section.")
        continue

      for row in section.table.rows:
        # pylint: disable=not-callable
        out_rdf = self.output_rdf_cls(metadata=metadata)
        # pylint: enable=not-callable

        for attr, value in zip(headers, row.values):
          if isinstance(getattr(out_rdf, attr), (str, unicode)):
            setattr(out_rdf, attr, value.svalue)
          else:
            setattr(out_rdf, attr, value.value)
        yield out_rdf


class VolatilityResultToExportedVolatilityHandleConverter(
    VolatilityResultConverter):
  """Converts VolatilityResult to ExportedVolatilityHandle."""

  mapping = {
      "offset_v": "offset",
      "pid": "pid",
      "handle": "handle",
      "access": "access",
      "obj_type": "type",
      "details": "path",
  }

  output_rdf_cls = rdfvalue.ExportedVolatilityHandle


class VolatilityResultToExportedVolatilityMutantConverter(
    VolatilityResultConverter):
  """Converts VolatilityResult to ExportedVolatilityMutant."""

  mapping = {
      "offset_p": "offset",
      "ptr_count": "ptr_count",
      "hnd_count": "handle_count",
      "mutant_signal": "signal",
      "mutant_thread": "thread",
      "cid": "cid",
      "mutant_name": "name",
  }

  output_rdf_cls = rdfvalue.ExportedVolatilityMutant


class ClientSummaryToExportedNetworkInterfaceConverter(ExportConverter):
  input_rdf_type = "ClientSummary"

  def Convert(self, metadata, client_summary, token=None):
    """Converts ClientSummary to ExportedNetworkInterfaces."""

    for interface in client_summary.interfaces:
      ip4_addresses = []
      ip6_addresses = []
      for addr in interface.addresses:
        if addr.address_type == addr.Family.INET:
          ip4_addresses.append(addr.human_readable_address)
        elif addr.address_type == addr.Family.INET6:
          ip6_addresses.append(addr.human_readable_address)
        else:
          raise ValueError("Invalid address type: %s", addr.address_type)

      yield ExportedNetworkInterface(
          metadata=metadata,
          mac_address=interface.mac_address.human_readable_address,
          ifname=interface.ifname,
          ip4_addresses=" ".join(ip4_addresses),
          ip6_addresses=" ".join(ip6_addresses))


class ClientSummaryToExportedClientConverter(ExportConverter):
  input_rdf_type = "ClientSummary"

  def Convert(self, metadata, unused_client_summary, token=None):
    return [ExportedClient(metadata=metadata)]


class RDFURNConverter(ExportConverter):
  """Follows RDFURN and converts its target object into a set of RDFValues.

  If urn points to a RDFValueCollection, RDFURNConverter goes through the
  collection and converts every value there. If urn points to an object
  with "STAT" attribute, it converts just that attribute.
  """

  input_rdf_type = "RDFURN"

  def Convert(self, metadata, urn, token=None):
    fd = aff4.FACTORY.Open(urn, token=token)
    if fd.Get(fd.Schema.TYPE) == "RDFValueCollection":
      for value in fd:
        for v in ConvertSingleValue(metadata, value, token=token):
          yield v
    else:
      stat_entry = fd.Get(fd.Schema.STAT)
      if stat_entry:
        for v in ConvertSingleValue(metadata, stat_entry, token=token):
          yield v


def GetMetadata(client, token=None):
  """Builds ExportedMetadata object for a given client id.

  Args:
    client: RDFURN of a client or VFSGRRClient object itself.
    token: Security token.

  Returns:
    ExportedMetadata object with metadata of the client.
  """

  if isinstance(client, rdfvalue.RDFURN):
    client_fd = aff4.FACTORY.Open(client, mode="r", token=token)
  else:
    client_fd = client

  metadata = ExportedMetadata()

  metadata.timestamp = rdfvalue.RDFDatetime().Now()

  metadata.client_urn = client_fd.urn
  metadata.client_age = client_fd.urn.age

  metadata.hostname = utils.SmartUnicode(
      client_fd.Get(client_fd.Schema.HOSTNAME, u""))

  metadata.os = utils.SmartUnicode(
      client_fd.Get(client_fd.Schema.SYSTEM, u""))

  metadata.uname = utils.SmartUnicode(
      client_fd.Get(client_fd.Schema.UNAME, u""))

  metadata.os_release = utils.SmartUnicode(
      client_fd.Get(client_fd.Schema.OS_RELEASE, u""))

  metadata.os_version = utils.SmartUnicode(
      client_fd.Get(client_fd.Schema.OS_VERSION, u""))

  metadata.usernames = utils.SmartUnicode(
      client_fd.Get(client_fd.Schema.USERNAMES, u""))

  metadata.mac_address = utils.SmartUnicode(
      client_fd.Get(client_fd.Schema.MAC_ADDRESS, u""))

  return metadata


def ConvertSingleValue(metadata, value, token=None, options=None):
  """Finds converters for a single value and converts it."""

  converters_classes = ExportConverter.GetConvertersByValue(value)
  if not converters_classes:
    raise NoConverterFound("No converters found for value: %s" % value)

  for converter_cls in converters_classes:
    converter = converter_cls(options=options)
    for v in converter.Convert(metadata, value, token=token):
      yield v


class RDFValueCollectionConverter(object):
  """Class used to convert sets of RDFValues to their exported versions."""

  def __init__(self, batch_size=None, threadpool_prefix=None,
               threadpool_size=None, callback_fn=None):
    """Constructor of RDFValueCollectionConverter.

    Args:
      batch_size: All the values will be processed in batches of this size.
      threadpool_prefix: Prefix that will be used in thread pool's threads
                         names.
      threadpool_size: Size of a thread pool that will be used.
                       If threadpool_size is 0, no threads will be used
                       and all conversions will be done in the current
                       thread.
      callback_fn: User-provided callback that will be called every time
                   a converted batch is ready.
    """
    super(RDFValueCollectionConverter, self).__init__()
    self.batch_size = batch_size or 1000
    self.threadpool_prefix = threadpool_prefix or "convert_collection"
    if threadpool_size is None:
      self.threadpool_size = 10
    else:
      self.threadpool_size = threadpool_size
    self.callback_fn = callback_fn

  def ProcessConvertedBatch(self, converted_batch):
    """Callback function that is called after every converted batch.

    It delegates handlign of the converted values to user-provided callback.

    Args:
      converted_batch: List or a generator of RDFValues.
    """
    if self.callback_fn:
      self.callback_fn(converted_batch)

  def _ProcessBatchTask(self, batch, converters_classes, options,
                        session_id, token):
    """Threadpool task method.

    Args:
      batch: Batch to process.
      converters_classes: List of converters to use.
      options: Export options to pass to converters.
      session_id: Session id to fill into the metadata.
      token: Security token.
    """
    msg_dict = {}
    for msg in batch:
      if msg.source not in msg_dict:
        msg_dict[msg.source] = []
      msg_dict[msg.source].append(msg)

    client_fds = aff4.FACTORY.MultiOpen(msg_dict.iterkeys(), mode="r",
                                        token=token)
    metadata_objects = [GetMetadata(client_fd, token=token)
                        for client_fd in client_fds]
    for metadata in metadata_objects:
      metadata.session_id = session_id

    converters = [cls(options) for cls in converters_classes]
    converted_batch = []

    batch_data = []
    for metadata in metadata_objects:
      try:
        batch_data.extend([(metadata, message.payload)
                           for message in msg_dict[metadata.client_urn]])
      except KeyError:
        pass

    for converter in converters:
      converted_batch.extend(
          converter.BatchConvert(batch_data, token=token))

    self.ProcessConvertedBatch(converted_batch)

  def Convert(self, collection, start_index=0, end_index=None, session_id=None,
              options=None, token=None):
    """Converts given collection to exported values.

    This method uses a threadpool to do the conversion in parallel. It
    blocks until everything is converted.

    Args:
      collection: Iterable object with GRRMessages whose payload
                  should be converted.
      start_index: Start from this index in the collection.
      end_index: Finish processing on the (index - 1) element of the
                 collection. If None, work till the end of the collection.
      session_id: to use in ExportedMetadata corresponding to values.
      options: Export options for the converters.
      token: Security token.

    Raises:
      NoConverterFound: if no available converters are found for the
                        values in given RDFValueCollection.

    Returns:
      Nothing. callback_fn should be used to process results.
    """
    if not collection:
      return

    try:
      total_batch_count = len(collection) / self.batch_size
    except TypeError:
      total_batch_count = -1

    pool = threadpool.ThreadPool.Factory(self.threadpool_prefix,
                                         self.threadpool_size)
    msg_iterator = itertools.islice(collection, start_index, end_index)

    pool.Start()
    try:
      converters_classes = None
      batch_index = 0
      batch = []
      for msg in msg_iterator:
        if not converters_classes:
          converters_classes = ExportConverter.GetConvertersByValue(
              msg.payload)
          if not converters_classes:
            raise NoConverterFound(
                "No converters found for value: %s" % msg)

        batch.append(msg)
        if len(batch) >= self.batch_size:
          logging.info("Processing batch %d out of %d", batch_index,
                       total_batch_count)

          pool.AddTask(target=self._ProcessBatchTask,
                       args=(batch, converters_classes,
                             options, session_id, token),
                       name="batch_%d" % batch_index)
          batch = []
          batch_index += 1

      if batch:
        pool.AddTask(target=self._ProcessBatchTask,
                     args=(batch, converters_classes,
                           options, session_id, token),
                     name="last_batch_%d" % batch_index)

    finally:
      pool.Stop()


class RDFValueCollectionListBasedConverter(RDFValueCollectionConverter):
  """RDFValueCollectionConverter that accumulates results in a list."""

  def __init__(self, *args, **kwargs):
    super(RDFValueCollectionListBasedConverter, self).__init__(*args, **kwargs)
    self.results = []

  def ProcessConvertedBatch(self, batch):
    self.results.extend(batch)

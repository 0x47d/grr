#!/usr/bin/env python
"""A simple thread pool for the Google Response Rig.

This file defines a simple thread pool that is used throughout this
project for parallelizing data store accesses. This thread pool is
rather lightweight and optimized to be used in combination with the
GRR data_store modules. It is not meant to be general purpose - if you
need a generalized thread pool, you should probably use a better
suited alternative implementation.

If during creation not all new worker threads can be spawned by the
ThreadPool, a log entry will be written but execution will continue
using a smaller pool of workers. In this case, consider reducing the
--threadpool_size.

Example usage:
>>> def PrintMsg(value):
>>>   print "Message: %s" % value
>>> for _ in range(10):
>>>   SharedPool().AddTask(PrintMsg, ("Hello World!", ))
>>> SharedPool().Join()

"""



import itertools
import Queue
import threading
import time


import logging

from grr.lib import stats
from grr.lib import utils


STOP_MESSAGE = "Stop message"


class Error(Exception):
  pass


class DuplicateThreadpoolError(Error):
  """Raised when a thread pool with the same name already exists."""


class Full(Error):
  """Raised when the threadpool is full."""


class _WorkerThread(threading.Thread):
  """The workers used in the ThreadPool class."""

  def __init__(self, threadpool_name, queue):
    """Initializer.

    This creates a new worker object for the ThreadPool class.

    Args:
      threadpool_name: The name of the thread pool this worker belongs to.
      queue: A Queue.Queue object that is used by the ThreadPool class to
          communicate with the workers. When a new task arrives, the ThreadPool
          notifies the workers by putting a message into this queue that has the
          format (target, args, name, queueing_time).

          target - A callable, the function to call.
          args - A tuple of positional arguments to target. Keyword arguments
                 are not supported.
          name - A name for this task. If None, it will be unique generated by
                 the threading library.
          queueing_time - The timestamp when this task was queued as returned by
                          time.time().

          Or, alternatively, the message in the queue can be STOP_MESSAGE
          which indicates that the worker should terminate.
    """

    if threadpool_name:
      name = threadpool_name + "_worker"
    else:
      # If not name, get a unique thread name and we'll disable stats.
      name = None
    threading.Thread.__init__(self, name=name)
    self._queue = queue
    self.daemon = True
    self.idle = True
    self.threadpool_name = threadpool_name

  def ProcessTask(self, target, args, name, queueing_time):
    """Processes the tasks."""

    if self.threadpool_name:
      time_in_queue = time.time() - queueing_time
      stats.STATS.RecordEvent(self.threadpool_name + "_queueing_time",
                              time_in_queue)

      start_time = time.time()
    try:
      target(*args)
    # We can't let a worker die because one of the tasks it has to process
    # throws an exception. Therefore, we catch every error that is
    # raised in the call to target().
    except Exception as e:  # pylint: disable=broad-except
      if self.threadpool_name:
        stats.STATS.IncrementCounter(self.threadpool_name + "_task_exceptions")
      logging.exception("Caught exception in worker thread (%s): %s",
                        name, str(e))

    if self.threadpool_name:
      total_time = time.time() - start_time
      stats.STATS.RecordEvent(self.threadpool_name + "_working_time",
                              total_time)

  def run(self):
    """This overrides the Thread.run method.

    This method checks in an endless loop if new tasks are available
    in the queue and processes them.
    """
    while True:
      if self.threadpool_name:
        self.idle = True

      task = self._queue.get()

      try:
        if self.threadpool_name:
          self.idle = False

        if task == STOP_MESSAGE:
          break

        self.ProcessTask(*task)

      finally:
        self._queue.task_done()


THREADPOOL = None


class ThreadPool(object):
  """A simple implementation of a thread pool used in GRR.

  This class implements a very simple thread pool intended for
  lightweight parallelization of data_store accesses.

  Note that this class should not be instantiated directly, but the Factory
  should be used.
  """
  # A global dictionary of pools, keyed by pool name.
  POOLS = {}
  factory_lock = threading.Lock()

  @classmethod
  def Factory(cls, name, num_threads):
    """Creates a new thread pool with the given name.

    If the thread pool of this name already exist, we just return the existing
    one. This allows us to have different pools with different characteristics
    used by different parts of the code, at the same time.

    Args:
      name: The name of the required pool.
      num_threads: The number of threads in the pool.

    Returns:
      A threadpool instance.
    """
    with cls.factory_lock:
      result = cls.POOLS.get(name)
      if result is None:
        cls.POOLS[name] = result = cls(name, num_threads)

      return result

  def __init__(self, name, num_threads):
    """This creates a new thread pool using num_threads workers.

    Args:
      name: A prefix to identify this thread pool in the exported stats.
      num_threads: The intended number of worker threads this pool should have.

    Raises:
      threading.ThreadError: If no threads can be spawned at all, ThreadError
                             will be raised.
      DuplicateThreadpoolError: This exception is raised if a thread pool with
                                the desired name already exists.
    """

    self._queue = Queue.Queue(2 * num_threads)
    self.num_threads = num_threads
    self.name = name
    self.started = False
    self.workers = []

    if self.name:
      if self.name in self.POOLS:
        raise DuplicateThreadpoolError(
            "A thread pool with the name %s already exists.", name)

      stats.STATS.RegisterGaugeMetric(self.name + "_outstanding_tasks", int)
      stats.STATS.SetGaugeCallback(self.name + "_outstanding_tasks",
                                   self._queue.qsize)

      stats.STATS.RegisterGaugeMetric(self.name + "_idle_threads", int)
      stats.STATS.SetGaugeCallback(self.name + "_idle_threads",
                                   lambda: self.idle_threads)

      stats.STATS.RegisterCounterMetric(self.name + "_task_exceptions")
      stats.STATS.RegisterEventMetric(self.name + "_working_time")
      stats.STATS.RegisterEventMetric(self.name + "_queueing_time")

  def __del__(self):
    if self.started:
      self.Stop()

  @property
  def idle_threads(self):
    return len([w for w in self.workers if w.idle])

  def Start(self):
    """This starts the worker threads."""
    if not self.started:
      self.workers = []
      self.started = True
      for thread_counter in range(self.num_threads):
        try:
          worker = _WorkerThread(self.name, self._queue)
          worker.start()
          self.workers.append(worker)
        except threading.ThreadError:
          if thread_counter == 0:
            logging.error(("Threadpool exception: "
                           "Could not spawn worker threads."))
            # If we cannot spawn any threads at all, bail out.
            raise
          else:
            logging.warning(("Threadpool exception: "
                             "Could only start %d threads."), thread_counter)
          break

  def Stop(self):
    """This stops all the worker threads."""
    if not self.started:
      logging.warning("Tried to stop a thread pool that was not running.")
      return

    # Send a stop message to all the workers.
    for _ in self.workers:
      self._queue.put(STOP_MESSAGE)

    self.started = False
    self.Join()

    # Wait for the threads to all exit now.
    for worker in self.workers:
      worker.join()

  def Available(self):
    return self._queue.qsize

  def AddTask(self, target, args, name="Unnamed task", blocking=True,
              inline=True):
    """Adds a task to be processed later.

    Args:
      target: A callable which should be processed by one of the workers.

      args: A tuple of arguments to target.

      name: The name of this task. Used to identify tasks in the log.

      blocking: If True we block until the task is finished, otherwise we raise
        Queue.Full

      inline: If set, process the task inline when the queue is full. This
        implies no blocking. Specifying inline helps if the worker tasks are
        blocked because it still ensures some progress is made. However, this
        can generally block the calling thread even after the threadpool is
        available again and therefore decrease efficiency.

    Raises:
      Full() if the pool is full and can not accept new jobs.
    """
    if self.num_threads == 0:
      target(*args)
      return

    if inline:
      blocking = False

    try:
      # Push the task on the queue but raise if unsuccessful.
      self._queue.put((target, args, name, time.time()), block=blocking)
    except Queue.Full:
      if inline:
        target(*args)
      else:
        raise Full()

  def Join(self):
    """Waits until all outstanding tasks are completed."""""
    self._queue.join()


class MockThreadPool(object):
  """A mock thread pool which runs all jobs serially."""

  def __init__(self, name, num_threads, ignore_errors=True):
    _ = name
    _ = num_threads
    self.ignore_errors = ignore_errors

  def AddTask(self, target, args, name="Unnamed task"):
    _ = name
    try:
      target(*args)
      # The real threadpool can not raise from a task. We emulate this here.
    except Exception as e:  # pylint: disable=broad-except
      logging.exception("MockThreadPool worker raised %s", e)
      if not self.ignore_errors:
        raise

  @classmethod
  def Factory(cls, name, num_threads):
    return cls(name, num_threads)

  def Start(self):
    pass

  def Stop(self):
    pass

  def Join(self):
    pass


class BatchConverter(object):
  """Generic class that does multi-threaded values conversion.

  BatchConverter converts a set of values to a set of different values in
  batches using a threadpool.
  """

  def __init__(self, batch_size=1000, threadpool_prefix="batch_processor",
               threadpool_size=10):
    """BatchProcessor constructor.

    Args:
      batch_size: All the values will be processed in batches of this size.
      threadpool_prefix: Prefix that will be used in thread pool's threads
                         names.
      threadpool_size: Size of a thread pool that will be used.
                       If threadpool_size is 0, no threads will be used
                       and all conversions will be done in the current
                       thread.
    """
    super(BatchConverter, self).__init__()
    self.batch_size = batch_size
    self.threadpool_prefix = threadpool_prefix
    self.threadpool_size = threadpool_size

  def ConvertBatch(self, batch):
    """ConvertBatch is called for every batch to do the conversion.

    Args:
      batch: Batch to convert.
    Returns:
      List with converted values.
    """
    raise NotImplementedError()

  def Convert(self, values, start_index=0, end_index=None):
    """Converts given collection to exported values.

    This method uses a threadpool to do the conversion in parallel. It
    blocks until everything is converted.

    Args:
      values: Iterable object with values to convert.
      start_index: Start from this index in the collection.
      end_index: Finish processing on the (index - 1) element of the
                 collection. If None, work till the end of the collection.

    Returns:
      Nothing. ConvertedBatch() should handle the results.
    """
    if not values:
      return

    try:
      total_batch_count = len(values) / self.batch_size
    except TypeError:
      total_batch_count = -1

    pool = ThreadPool.Factory(self.threadpool_prefix,
                              self.threadpool_size)
    val_iterator = itertools.islice(values, start_index, end_index)

    pool.Start()
    try:
      for batch_index, batch in enumerate(utils.Grouper(val_iterator,
                                                        self.batch_size)):
        logging.info("Processing batch %d out of %d", batch_index,
                     total_batch_count)

        pool.AddTask(target=self.ConvertBatch,
                     args=(batch,), name="batch_%d" % batch_index,
                     inline=False)

    finally:
      pool.Stop()
